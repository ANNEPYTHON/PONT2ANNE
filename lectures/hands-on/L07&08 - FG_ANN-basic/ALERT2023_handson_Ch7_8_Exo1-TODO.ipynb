{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ANNEPYTHON/PONT2ANNE/blob/main/lectures/hands-on/L07%2608%20-%20FG_ANN-basic/ALERT2023_handson_Ch7_8_Exo1-TODO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvfMXRY5jx5M"
      },
      "source": [
        "# **ALERT 2023**\n",
        "\n",
        "## **Artificial Neural Networks - Chapters 7 and 8**\n",
        "\n",
        "\n",
        "Author: Filippo Gatti"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrRtpSJ_eLpV"
      },
      "source": [
        "### Disclaimer\n",
        "\n",
        "This hands-on notebook is devoted to **artificial Neural Networks** ($\\mathcal{NN}$) and it covers chapters 7 and 8.\n",
        "\n",
        "In the following, the code cells introduced by a tag **[TODO]** are meant to be completed by you!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rVRiwgPxqEl7"
      },
      "outputs": [],
      "source": [
        "# basic packages\n",
        "from IPython.display import Image\n",
        "import numpy as np\n",
        "import scipy\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "from mpl_toolkits.mplot3d import axes3d, Axes3D\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "# This is required only for notebooks\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zzfhscvzu4TW"
      },
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8bJbXG7miOv"
      },
      "source": [
        "## **Preliminaries**: practicing with `python3`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRbjh8FDlrwx"
      },
      "source": [
        "Before getting started, a few general hints:\n",
        "\n",
        "1. `lambda` constructors: they are used in `python` to define anonymous functions ([see this link](https://realpython.com/python-lambda/))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCYW6lnX0rRV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "arr = np.array([1.5, 2.8, 3.1])\n",
        "scale = lambda x: x * 3\n",
        "scale(arr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlwVHKC4QCgC"
      },
      "source": [
        "2. Handle `pandas` dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPA9JSHJQUdn"
      },
      "outputs": [],
      "source": [
        "\n",
        "Image(url=\"https://www.tutorialspoint.com/python_pandas/images/structure_table.jpg\",width=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ta5_ta5iQefI"
      },
      "outputs": [],
      "source": [
        "# Empty dataframe\n",
        "df = pd.DataFrame()\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zseq-qWdQgEJ"
      },
      "outputs": [],
      "source": [
        "# Basic database\n",
        "data = [1,2,3,4,5]\n",
        "df = pd.DataFrame(data)\n",
        "print(df.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfzt1ZFAQkyd"
      },
      "outputs": [],
      "source": [
        "# Database with column labels\n",
        "data = [['Alex',10],['Bob',12],['Clarke',13]]\n",
        "df = pd.DataFrame(data,columns=['Name','Age'])\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnHJWEViQwCW"
      },
      "outputs": [],
      "source": [
        "# Repartitioning data into dataframe\n",
        "data = [['Alex',10],['Bob',12],['Clarke',13]]\n",
        "df = pd.DataFrame(data,columns=['Name','Age'],dtype=float)\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sI3X_pS6Qwjp"
      },
      "outputs": [],
      "source": [
        "# Deal with indices\n",
        "data = {'Name':['Tom', 'Jack', 'Steve', 'Ricky'],'Age':[28,34,29,42]}\n",
        "df = pd.DataFrame(data, index=['rank1','rank2','rank3','rank4'])\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXEd3Abh3dxD"
      },
      "source": [
        "3. plot data: ([see this link](https://matplotlib.org/tutorials/introductory/usage.html#sphx-glr-tutorials-introductory-usage-py))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjLz5ga_8crQ"
      },
      "outputs": [],
      "source": [
        "x = np.linspace(0, 2, 100)\n",
        "plt.plot(x, x, label='linear')\n",
        "plt.plot(x, x**2, label='quadratic')\n",
        "plt.plot(x, x**3, label='cubic')\n",
        "plt.xlabel('x label')\n",
        "plt.ylabel('y label')\n",
        "plt.title(\"Simple Plot\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqdFye6vWfYM"
      },
      "source": [
        "# **Exercise 1** Non-linear regressions of real functions with $\\mathcal{MLP}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHcUGwPMbAwT"
      },
      "source": [
        "The aim of this task is to get acquainted with non-linear polynomial regressions. In this task, the following *kindergarten* equation is considered:\n",
        "\n",
        "$$f(x) = \\sin(30 \\cdot (x-0.9)^4)\\cos(2 \\cdot (x-0.9))+\\frac{x-0.9}{2}$$\n",
        "\n",
        "Given the equation above, solve the following issues:\n",
        "\n",
        "## - Step 1 **[TODO]**: Plot $f(x)$ and evaluate it at $N$=100 random points $x_i\\sim\\mathcal{U}\\left[0,1\\right]$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 774
        },
        "id": "SsO6NkXVoIsV",
        "outputId": "8c3e14e1-4eb4-455a-f1f9-8fafa4d2de29"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "x, y, and format string must not be None",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-702094c70309>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"black\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"teal\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"yellowgreen\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gold\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"darkorange\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tomato\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m )\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_plot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_plot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ground truth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# plot training points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \"\"\"\n\u001b[1;32m   1687\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1689\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m             yield from self._plot_args(\n\u001b[0m\u001b[1;32m    312\u001b[0m                 this, kwargs, ambiguous_fmt_datakey=ambiguous_fmt_datakey)\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;31m# element array of None which causes problems downstream.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x, y, and format string must not be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: x, y, and format string must not be None"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy\n",
        "# Grant reproductibility\n",
        "np.random.seed(0)\n",
        "N = 10\n",
        "x_plot = np.linspace(0, 1, N*100)\n",
        "rng = np.random.RandomState(0)\n",
        "x_train = np.sort(rng.choice(x_plot,\n",
        "                             size=N,\n",
        "                             replace=False))\n",
        "def f(x):\n",
        "    return # [TODO]\n",
        "y_train = f(x_train)\n",
        "\n",
        "# create 2D-array versions of these arrays to feed to transformers\n",
        "X_train = x_train[:, np.newaxis]\n",
        "X_plot = x_plot[:, np.newaxis]\n",
        "\n",
        "# plot function\n",
        "lw = 2\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_prop_cycle(\n",
        "    color=[\"black\", \"teal\", \"yellowgreen\", \"gold\", \"darkorange\", \"tomato\"]\n",
        ")\n",
        "ax.plot(x_plot, f(x_plot), linewidth=lw, label=\"ground truth\")\n",
        "\n",
        "# plot training points\n",
        "ax.scatter(x_train, y_train, label=\"training points\")\n",
        "ax.set_xlabel(\"x\")\n",
        "ax.set_ylabel(\"f(x)\")\n",
        "ax.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihkjOEf_cLvW"
      },
      "source": [
        "\n",
        "\n",
        "## - Step 2 **[TODO]**: Fit the selected points for different polynomial orders (hint: 3,4,5,... or piece-wise polynomials). Show the fitting improvements obtained when changing the polynomial order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lz6aBTurhLIX"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures, SplineTransformer\n",
        "\n",
        "lw = 2\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_prop_cycle(\n",
        "    color=[\"black\", \"teal\", \"yellowgreen\",\n",
        "           \"gold\", \"darkorange\", \"tomato\",\n",
        "           \"skyblue\"]\n",
        ")\n",
        "ax.plot(x_plot, f(x_plot), linewidth=lw, label=\"ground truth\")\n",
        "\n",
        "# plot training points\n",
        "ax.scatter(x_train, y_train, label=\"training points\")\n",
        "\n",
        "# polynomial features\n",
        "alpha = 1e-10# penalty coefficient\n",
        "for degree in [3, 4, 5, 9, N]:\n",
        "    model = make_pipeline(PolynomialFeatures(degree),\n",
        "                          Ridge(alpha=alpha))\n",
        "    # [TODO]\n",
        "    # Hint: use the sklearn function `fit` and `predict`\n",
        "    ax.plot(x_plot, y_plot, label=f\"degree {degree}\")\n",
        "\n",
        "\n",
        "ax.legend()\n",
        "ax.set_xlim(0, 1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73L6MJjZhKU9"
      },
      "source": [
        "\n",
        "## - Step 3 **[TODO]**: How the fit improves when considering 100, 1000 random points?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxKo-1bsoy_l"
      },
      "outputs": [],
      "source": [
        "N = 1000\n",
        "x_train = np.sort(rng.choice(x_plot,\n",
        "                             size=N,\n",
        "                             replace=False))\n",
        "y_train = f(x_train)\n",
        "X_train = x_train[:, np.newaxis]\n",
        "\n",
        "lw = 2\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_prop_cycle(\n",
        "    color=[\"black\", \"teal\", \"yellowgreen\",\n",
        "           \"gold\", \"darkorange\", \"tomato\",\n",
        "           \"skyblue\"]\n",
        ")\n",
        "ax.plot(x_plot, f(x_plot), linewidth=lw, label=\"ground truth\")\n",
        "\n",
        "# plot training points\n",
        "ax.scatter(x_train, y_train, label=\"training points\")\n",
        "# polynomial features\n",
        "alpha =0 # penalty coefficient\n",
        "for degree in [3, 4, 5, 9, N//2, N-1]:\n",
        "    model = make_pipeline(PolynomialFeatures(degree),\n",
        "                          Ridge(alpha=alpha))\n",
        "    # [TODO]\n",
        "    # Hint: use the sklearn function `fit` and `predict`\n",
        "    ax.plot(x_plot, y_plot, label=f\"degree {degree}\")\n",
        "\n",
        "\n",
        "ax.legend()\n",
        "ax.set_xlim(0, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bv66hKpozO-"
      },
      "source": [
        "## - Step 4 **[TODO]**:  Design a $\\mathcal{MLP}$ to fit the curve sampled with 10, 100, 1000 points respectively. Use the least number of layers and neurons possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWM9YcBFHTS5"
      },
      "source": [
        "## - Step 4.1 **[TODO]**:  create and split dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmvXrMPbHO8m"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAEE1-ops902"
      },
      "outputs": [],
      "source": [
        "'''[TODO] Create dataset'''\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "m = torch.distributions.Uniform(torch.zeros(1),\n",
        "                                torch.ones(1))\n",
        "\n",
        "N = 1000\n",
        "X = m.sample((N,)).to(torch.float32)\n",
        "\n",
        "# split into train and test sets\n",
        "# Hint: use the sklearn function `train_test_split`\n",
        "# [TODO]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHll11ZbI4U7"
      },
      "source": [
        "## - Step 4.2 **[TODO]**:  design the $\\mathcal{MLP}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OvUF4A_8_ta"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "'''[TODO] Design MLP'''\n",
        "\n",
        "fan_in = 4\n",
        "h_theta = nn.Sequential(\n",
        "    # [TODO]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyVUTJR8JBvN"
      },
      "source": [
        "## - Step 4.3 **[TODO]**:  choose the optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXtCCbeSwJCC"
      },
      "outputs": [],
      "source": [
        "'''[TODO] Optimization setup'''\n",
        "# Define the loss function\n",
        "loss_fn = # [TODO]\n",
        "\n",
        "# number of epochs\n",
        "n_e = 800\n",
        "\n",
        "# batch size and batch per epochs\n",
        "batch_size = 200\n",
        "batches_per_epoch = len(X_train) // batch_size\n",
        "\n",
        "# Optimizer\n",
        "learning_rate = 0.0001\n",
        "beta1 = 0.9 # Adam coefficients\n",
        "beta2 = 0.999 # Adam coefficients\n",
        "epsilon = 1e-8 # tolerance\n",
        "optimizer = # [TODO]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2x-Z8NuJNDm"
      },
      "source": [
        "## - Step 4.4 **[TODO]**:  train the $\\mathcal{MLP}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFQeWdXIv9Hv"
      },
      "outputs": [],
      "source": [
        "'''[TODO] Train the hidden-layer MLP'''\n",
        "import tqdm\n",
        "train_loss_hist = []\n",
        "test_loss_hist = []\n",
        "for epoch in range(n_e):\n",
        "    epoch_loss = []\n",
        "    # set model in training mode\n",
        "    h_theta.train()\n",
        "\n",
        "    with tqdm.trange(batches_per_epoch, unit=\"batch\", mininterval=0) as bar:\n",
        "        bar.set_description(f\"Epoch {epoch}\")\n",
        "        for i in bar:\n",
        "            # take a batch\n",
        "            start = i * batch_size\n",
        "            X_batch = X_train[start:start+batch_size]\n",
        "            y_batch = y_train[start:start+batch_size]\n",
        "            # infer (forward)\n",
        "            y_pred = h_theta(X_batch).squeeze()\n",
        "            # compute the loss\n",
        "            loss = # [TODO]\n",
        "            # reset previously saved gradients and empty the optimizer memory\n",
        "            optimizer.zero_grad()\n",
        "            # run backward propagation\n",
        "            # [TODO]\n",
        "            # update weights\n",
        "            # [TODO]\n",
        "            # compute and store metrics\n",
        "            epoch_loss.append(float(loss))\n",
        "\n",
        "    # set model in evaluation mode to infer the class in the test set\n",
        "    # without storing gradients for brackprop\n",
        "    h_theta.eval()\n",
        "    # infer the class over the test set\n",
        "    y_pred = h_theta(X_test).squeeze()\n",
        "    acc = float(loss_fn(y_pred, y_test))\n",
        "    train_loss_hist.append(np.mean(epoch_loss))\n",
        "    test_loss_hist.append(acc)\n",
        "    # print(f\"Epoch {epoch} validation: MSE={acc:.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WHopbt0JR7L"
      },
      "source": [
        "## - Step 4.5 **[TODO]**:  plot the learning curves and the comparison between test data and prediction from trained $\\mathcal{MLP}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaNtL74y14Oy"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(4,4))\n",
        "ax.semilogy(train_loss_hist,\n",
        "            color='b',\n",
        "            linewidth=1,\n",
        "            label=r\"train $h_{\\mathbf{\\theta}}(\\mathbf{x})$\")\n",
        "# [TODO] Plot test curve (use label=r\"test $h_{\\mathbf{\\theta}}(\\mathbf{x})$\"))\n",
        "ax.set_xlabel(\"epoch\")\n",
        "ax.set_ylabel(r\"$\\Vert f(x)-h_\\theta(x) \\Vert^2$\")\n",
        "ax.set_xlim(0,n_e)\n",
        "ax.set_ylim(1e-6,1e0)\n",
        "ax.legend(frameon=False, loc='upper center', bbox_to_anchor=(0.5, 1.0),ncol=2,)\n",
        "fig.savefig(\"mse_fx_compare.png\", dpi=300, bbox_inches=\"tight\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDEVttiz21hw"
      },
      "outputs": [],
      "source": [
        "h_theta.eval()\n",
        "\n",
        "lw = 2\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_prop_cycle(\n",
        "    color=[\"black\", \"teal\", \"yellowgreen\",\n",
        "           \"gold\", \"darkorange\", \"tomato\",\n",
        "           \"skyblue\"]\n",
        ")\n",
        "ax.plot(x_plot, f(x_plot), linewidth=lw, label=\"ground truth\")\n",
        "\n",
        "ax.scatter(X_test.cpu().numpy(),h_theta(X_test).detach().cpu().numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NR051wadwn7C"
      },
      "source": [
        "# **Exercise 2**: Approximate an approximately-radial function with $\\mathcal{MLP}$ (see Section 2.4, Chapter 7)\n",
        "\n",
        "## **Quick facts**\n",
        "\n",
        "\n",
        "- Despite the fact that Universal approximation theorem for a 1-hidden-layered perceptron (see Theorem 24, Chapter 8) proves the universal approximation capability of a 1-hidden-layer $\\mathcal{MLP}$, provided that enough neurons are considered, this result is quite hard to exploit in real applications, since the number of neurons can easily become too large to handle from a computational standpoint.\n",
        "\n",
        "- Eldan and Shamir [1] showed that it exists an approximately radial function $\\varphi(\\mathbf{x}):\\mathbb{R}^{d_X}\\to\\mathbb{R}$, $\\varphi: \\mathbf{x}\\mapsto\\varphi(\\Vert\\mathbf{x}\\Vert)$ that can be approximated by a ``small`` (bounded number of neurons) 2-hidden-layers $\\mathcal{MLP}$ with arbitrary accuracy, but that cannot be approximated by a 1-hidden-layer $\\mathcal{MLP}$ below a certain accuracy, unless the number of neurons $N_K$ grows exponentially with $d_X$.\n",
        "\n",
        "- In particular, this results is valid for any activation function $g$ and with no further constraint on the weights and biases adopted in the $\\mathcal{MLP}$ (on the contrary, the Universal Approximation Theorem requires that the high-frequency components $\\Vert \\mathbf{w}_n\\Vert$ are smaller than a constant).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### *Bibliography*:\n",
        "\n",
        "[1] Eldan, R.; Shamir, O. The Power of Depth for Feedforward Neural Networks. In Workshop and Conference Proceedings; PMLR, 2016; Vol. 49, pp 1--34. url: https://proceedings.mlr.press/v49/eldan16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7j1SBUCH-AZ7"
      },
      "source": [
        "## **Questions**\n",
        "\n",
        "- What kind of functions cannot be approximated with a *reasonable* accuracy by a $\\mathcal{MLP}$ of $N_\\ell$ layers?\n",
        "\n",
        "- How many neurons should be considered for each layer?\n",
        "\n",
        "- What is the effect of having a number of layers that is higher than the number of neurons per layer?\n",
        "\n",
        "## **Learning Outcomes**\n",
        "\n",
        "- This results proves that increasing the depth of the $\\mathcal{MLP}$ widens the approximation capability of the $\\mathcal{MLP}$ and that the depth of the $\\mathcal{MLP}$ should be privileged with the respect to its layer dimension (but being careful to avoid vanishing\n",
        "gradient problems)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ny6N7jz-HOs"
      },
      "source": [
        "## **Objective**\n",
        "\n",
        "In the following, you are asked to conceive a multi-layer feed-forward $\\mathcal{MLP}$ to approximate the following approximately-radial function and compare the accuracy obtained with 1-hidden layer and with 2-hidden layers. The approximately-radial function is defined as:\n",
        "\n",
        "$$\\varphi(\\mathbf{x}) = \\left(\\frac{R_{d_X}}{\\Vert\\mathbf{x}\\Vert}\\right)^{\\frac{d_X}{2}}J_{\\frac{d_X}{2}}(2\\pi R_{d_X}\\Vert\\mathbf{x}\\Vert)  =\\int_{\\mathbf{w}:\\Vert\\mathbf{w}\\Vert\\le R_d} e^{-2\\pi i \\langle \\mathbf{x}, \\mathbf{w}\\rangle} d\\mathbf{w}\\phantom{}^{(1)}\n",
        "$$\n",
        "\n",
        "with $J_{\\frac{d_X}{2}}(2\\pi R_{d_X}\\Vert\\mathbf{x}\\Vert)$ the Bessel function of first kind of order $\\frac{d_X}{2}$. In the following, consider $d_X=2$.\n",
        "\n",
        "**$\\phantom{}^{(1)}$Note**: The approximately radial function is the inverse Fourier transform of the indicator function on a unit volume euclidean ball $B_{R_{d_X}}(0)$, of radius $R_{d_X}$ such that its volume $V_{d_X}(R_{d_X}) = 1$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuepuQKsCv8L"
      },
      "source": [
        "## - Step 1: compute and plot the approximately-radial function $\\varphi$ on, restricted to a compact set $\\mathcal{X}_{\\square}=\\left[-l_x,l_x\\right]\\times\\left[-l_y,l_y\\right]\\subset\\mathbb{R}^2$ over a discrete regular grid of $n_x\\times n_y$ points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LYWqFGgksDx"
      },
      "outputs": [],
      "source": [
        "'''Define grid in R2'''\n",
        "lx = 10.0\n",
        "ly = 10.0\n",
        "x_lim = np.array([-lx,lx], dtype=np.float64) # x-bounds\n",
        "y_lim = np.array([-ly,ly], dtype=np.float64) # y-bounds\n",
        "\n",
        "nx = 1001 # number of grid points along x\n",
        "ny = 1001 # number of grid points along y\n",
        "# vector of coordinates along x\n",
        "xv = np.linspace(x_lim[0], x_lim[1], nx, endpoint=True)\n",
        "# vector of coordinates along y\n",
        "yv = np.linspace(y_lim[0], y_lim[1], ny, endpoint=True)\n",
        "# define mesh grid\n",
        "yg, xg = np.meshgrid(yv,xv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yjldGWsEwvP"
      },
      "source": [
        "## - Step 2: evaluate $\\varphi$ over the grid and plot it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjNevvVGks5P"
      },
      "outputs": [],
      "source": [
        "'''Define varphi(x,y) over the grid'''\n",
        "from scipy.special import jv # Bessel function\n",
        "d_X = 2 # dimension of the problem\n",
        "R_d = 0.2 # radius\n",
        "phi = (R_d/(np.sqrt(xg**2+yg**2)))**(int(d_X//2))*jv(int(d_X//2),\n",
        "                                                     2.0*np.pi*R_d*np.sqrt(xg**2+\n",
        "                                                                           yg**2))\n",
        "fig = plt.figure(figsize=(4,4),)\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(xg, yg, phi, cmap=cm.jet)\n",
        "ax.set_xlabel(r\"$x$\")\n",
        "ax.set_ylabel(r\"$y$\")\n",
        "ax.set_zlabel(r\"$\\varphi(x,y)$\")\n",
        "fig.savefig(\"radial_function.png\", dpi=300, bbox_inches=\"tight\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKUzqPK9Gsxz"
      },
      "source": [
        "## - Step 3 **[TODO]**: create the training and test datasets by uniformly sampling the function $\\varphi$ over $\\mathcal{X}_\\square$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NefRB4EZk0s_"
      },
      "outputs": [],
      "source": [
        "''' [TODO] Create uniformly random grid of points for training'''\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Grant reproductibility\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Hint: use the package torch.distributions\n",
        "# https://pytorch.org/docs/stable/distributions.html\n",
        "# for uniform distribution\n",
        "m = torch.distributions # [TODO]\n",
        "X = m.sample((4000,)).to(torch.float32)\n",
        "y = (R_d/(np.sqrt(X[:,0]**2+X[:,1]**2)))**(int(d_X//2))*jv(int(d_X//2),\n",
        "                                                           2.0*np.pi*R_d*np.sqrt(\n",
        "                                                              X[:,0]**2+\n",
        "                                                              X[:,1]**2))\n",
        "\n",
        "# split into train and test sets\n",
        "# Hint: use the sklearn function `train_test_split`\n",
        " # [TODO]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAw0obKNKwpZ"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(9,4),)\n",
        "ax = fig.add_subplot(121, projection='3d')\n",
        "ax.plot_surface(xg, yg, phi, cmap=cm.jet, alpha=0.3)\n",
        "ax.scatter(X_train[:,0], X_train[:,1], y_train, color='k', s=5)\n",
        "ax.set_xlabel(r\"$x$\")\n",
        "ax.set_ylabel(r\"$y$\")\n",
        "ax.set_zlabel(r\"$\\varphi(x,y)$\")\n",
        "ax.set_title(\"Train dataset\")\n",
        "\n",
        "ax = fig.add_subplot(122, projection='3d')\n",
        "ax.plot_surface(xg, yg, phi, cmap=cm.jet, alpha=0.3)\n",
        "ax.scatter(X_test[:,0], X_test[:,1], y_test, color='k', s=5)\n",
        "\n",
        "ax.set_xlabel(r\"$x$\")\n",
        "ax.set_ylabel(r\"$y$\")\n",
        "ax.set_zlabel(r\"$\\varphi(x,y)$\")\n",
        "ax.set_title(\"Test dataset\")\n",
        "fig.savefig(\"train_test_radial_function.png\", dpi= 300, bbox_inches=\"tight\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GA3GAzrkH6-k"
      },
      "source": [
        "## - Step 4 **[TODO]**: Define a 1-hidden layer $\\mathcal{MLP}$ $h_\\theta^{(1)}$ and a 2-hidden layers $\\mathcal{MLP}$ $h_\\theta^{(2)}$ for further comparisons, using the `torch.nn` package. Adopt an adequate number of of hidden neurons for both cases.\n",
        "\n",
        "**Note**: use $ReLU$ activation functions and linear output layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8Y-hKUgDKlE"
      },
      "outputs": [],
      "source": [
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eqk2UGjkvev"
      },
      "outputs": [],
      "source": [
        "'''[TODO] Define 1-hidden-layer MLP selecting an adequate fan_in'''\n",
        "fan_in = 10000\n",
        "h_theta1 = nn.Sequential(\n",
        " # [TODO]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUaK3Bq-kyvn"
      },
      "outputs": [],
      "source": [
        "'''[TODO] Define 2-hidden-layer MLP'''\n",
        "fan_in = 100\n",
        "h_theta2 = nn.Sequential(\n",
        " # [TODO]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93WvyG4xI9T4"
      },
      "source": [
        "## - Step 5 **[TODO]**: Define the best optimizer for both $h_\\theta^{(1)}$ and $h_\\theta^{(2)}$. Choose the learning rate accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cobc_jmUIoHw"
      },
      "outputs": [],
      "source": [
        "''' [TODO] Define the optimizer for 1-hidden-layer MLP'''\n",
        "learning_rate1 = 0.0001\n",
        "optimizer1 =  # [TODO]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTzr2nHhHuf4"
      },
      "outputs": [],
      "source": [
        "''' [TODO] Define the optimizer for 2-hidden-layers MLP'''\n",
        "learning_rate2 = 0.0001\n",
        "optimizer2 =  # [TODO]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bg96YTMlK7rf"
      },
      "source": [
        "## - Step 6 **[TODO]**: setup the gradient descent strategy and the loss function. Optional: initialize the weights according to the best strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDzjZxEMk5C3"
      },
      "outputs": [],
      "source": [
        "''' [TODO] Gradient descent set up and loss function'''\n",
        "# Define the loss function\n",
        "loss_fn =  # [TODO]\n",
        "\n",
        "# number of epochs\n",
        "n_e = 1000\n",
        "\n",
        "# batch size and batch per epochs\n",
        "batch_size = 200\n",
        "batches_per_epoch = len(X_train) // batch_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HFP4U7cLr__"
      },
      "source": [
        "## - Step 7 **[TODO]**: Train $h_\\theta^{(1)}$ and $h_\\theta^{(2)}$ and track the approximation error for training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_q9Potwk7LP"
      },
      "outputs": [],
      "source": [
        "''' [TODO] Train 1-hidden-layer MLP'''\n",
        "import tqdm\n",
        "train_loss_hist1 = []\n",
        "test_loss_hist1 = []\n",
        "for epoch in range(n_e):\n",
        "    epoch_loss = []\n",
        "    # set model in training mode\n",
        "    h_theta1.train()\n",
        "\n",
        "    with tqdm.trange(batches_per_epoch, unit=\"batch\", mininterval=0) as bar:\n",
        "        bar.set_description(f\"Epoch {epoch}\")\n",
        "        for i in bar:\n",
        "            # take a batch\n",
        "            start = i * batch_size\n",
        "            X_batch = X_train[start:start+batch_size]\n",
        "            y_batch = y_train[start:start+batch_size]\n",
        "            # infer (forward)\n",
        "            y_pred =  # [TODO]\n",
        "            # compute the loss\n",
        "            loss =  # [TODO]\n",
        "            # reset previously saved gradients and empty the optimizer memory\n",
        "            optimizer1.zero_grad()\n",
        "            # run backward propagation\n",
        "            loss.backward()\n",
        "            # update weights\n",
        "            optimizer1.step()\n",
        "            # compute and store metrics\n",
        "            epoch_loss.append(float(loss))\n",
        "\n",
        "    # set model in evaluation mode to infer the class in the test set\n",
        "    # without storing gradients for brackprop\n",
        "    h_theta1.eval()\n",
        "    # infer the class over the test set\n",
        "    y_pred = h_theta1(X_test).squeeze()\n",
        "    acc = float(loss_fn(y_pred, y_test))\n",
        "    train_loss_hist1.append(np.mean(epoch_loss))\n",
        "    test_loss_hist1.append(acc)\n",
        "    print(f\"Epoch {epoch} validation: MSE={acc:.1f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aESnHNzk9kn"
      },
      "outputs": [],
      "source": [
        "'''[TODO] Train 2-hidden-layer MLP'''\n",
        "import tqdm\n",
        "train_loss_hist2 = []\n",
        "test_loss_hist2 = []\n",
        "for epoch in range(n_e):\n",
        "    epoch_loss = []\n",
        "    # set model in training mode\n",
        "    h_theta2.train()\n",
        "\n",
        "    with tqdm.trange(batches_per_epoch, unit=\"batch\", mininterval=0) as bar:\n",
        "        bar.set_description(f\"Epoch {epoch}\")\n",
        "        for i in bar:\n",
        "            # take a batch\n",
        "            start = i * batch_size\n",
        "            X_batch = X_train[start:start+batch_size]\n",
        "            y_batch = y_train[start:start+batch_size]\n",
        "            # infer (forward)\n",
        "            y_pred = h_theta2(X_batch).squeeze()\n",
        "            # compute the loss\n",
        "            loss = loss_fn(y_pred, y_batch)\n",
        "            # reset previously saved gradients and empty the optimizer memory\n",
        "            # [TODO]\n",
        "            # run backward propagation\n",
        "            # [TODO]\n",
        "            # update weights\n",
        "            optimizer2.step()\n",
        "            # compute and store metrics\n",
        "            epoch_loss.append(float(loss))\n",
        "\n",
        "    # set model in evaluation mode to infer the class in the test set\n",
        "    # without storing gradients for brackprop\n",
        "    h_theta2.eval()\n",
        "    # infer the class over the test set\n",
        "    y_pred = h_theta2(X_test).squeeze()\n",
        "    acc = float(loss_fn(y_pred, y_test))\n",
        "    train_loss_hist2.append(np.mean(epoch_loss))\n",
        "    test_loss_hist2.append(acc)\n",
        "    # print(f\"Epoch {epoch} validation: Accuracy={acc:.1f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_W1zPfRnk_tb"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(4,4))\n",
        "ax.semilogy(train_loss_hist1,\n",
        "            color='b',\n",
        "            linewidth=1,\n",
        "            label=r\"train $h^{1}_{\\mathbf{\\theta}}(\\mathbf{x})$\")\n",
        "ax.semilogy(test_loss_hist1,\n",
        "            color='r',\n",
        "            linewidth=1,\n",
        "            label=r\"test $h^{1}_{\\mathbf{\\theta}}(\\mathbf{x})$\")\n",
        "ax.semilogy(train_loss_hist2,\n",
        "            color='steelblue',\n",
        "            linewidth=3,\n",
        "            label=r\"train $h^{2}_{\\mathbf{\\theta}}(\\mathbf{x})$\")\n",
        "ax.semilogy(test_loss_hist2,\n",
        "            color='orange',\n",
        "            linewidth=3,\n",
        "            label=r\"test $h^{2}_{\\mathbf{\\theta}}(\\mathbf{x})$\")\n",
        "ax.set_xlabel(\"epoch\")\n",
        "ax.set_ylabel(r\"$\\Vert \\varphi(x,y)-h_{\\mathbf{\\theta}}(\\mathbf{x}) \\Vert^2$\")\n",
        "ax.set_xlim(0,n_e)\n",
        "ax.set_ylim(1e-6,1e0)\n",
        "ax.legend(frameon=False, loc='upper center', bbox_to_anchor=(0.5, 1.0),ncol=2,)\n",
        "fig.savefig(\"mse_radial_compare.png\", dpi=300, bbox_inches=\"tight\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zWTAbXvlB3f"
      },
      "outputs": [],
      "source": [
        "h_theta2.eval()\n",
        "fig = plt.figure(figsize=(4,4),)\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "# ax.plot_surface(xg, yg, phi, cmap=cm.jet, alpha=0.3)\n",
        "ax.scatter(X_train[:,0], X_train[:,1], y_train, color='k', s=30)\n",
        "ax.scatter(X_test[:,0], X_test[:,1], y_test, color='orange', s=30)\n",
        "ax.scatter(X_train[:,0], X_train[:,1],\n",
        "           h_theta2(X_train).detach().cpu().numpy(), color='k', s=30, marker=\"x\")\n",
        "ax.scatter(X_test[:,0], X_test[:,1],\n",
        "           h_theta2(X_test).detach().cpu().numpy(), color='orange', s=30, marker=\"x\")\n",
        "\n",
        "ax.set_xlabel(r\"$x$\")\n",
        "ax.set_ylabel(r\"$y$\")\n",
        "ax.set_zlabel(r\"$\\varphi(x,y)$\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSHEVb3oMN47"
      },
      "source": [
        "## - Step 8 **[TODO]**: what is the best traded-off between total number of weights, training time, approximation error?\n",
        "\n",
        "Try different solutions !"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
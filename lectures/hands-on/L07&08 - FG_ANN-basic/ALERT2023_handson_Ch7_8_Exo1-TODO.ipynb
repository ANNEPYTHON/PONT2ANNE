{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ANNEPYTHON/PONT2ANNE/blob/main/lectures/hands-on/L07%2608%20-%20FG_ANN-basic/ALERT2023_handson_Ch7_8_Exo1-TODO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvfMXRY5jx5M"
      },
      "source": [
        "# **ALERT 2023**\n",
        "\n",
        "## **Artificial Neural Networks - Chapters 7 and 8**\n",
        "\n",
        "\n",
        "Author: Filippo Gatti"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrRtpSJ_eLpV"
      },
      "source": [
        "### Disclaimer\n",
        "\n",
        "This hands-on notebook is devoted to **artificial Neural Networks** ($\\mathcal{NN}$) and it covers chapters 7 and 8.\n",
        "\n",
        "In the following, the code cells introduced by a tag **[TODO]** are meant to be completed by you!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVRiwgPxqEl7"
      },
      "outputs": [],
      "source": [
        "# basic packages\n",
        "from IPython.display import Image\n",
        "import numpy as np\n",
        "import scipy\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "from mpl_toolkits.mplot3d import axes3d, Axes3D\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "# This is required only for notebooks\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zzfhscvzu4TW"
      },
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8bJbXG7miOv"
      },
      "source": [
        "## **Preliminaries**: practicing with `python3`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRbjh8FDlrwx"
      },
      "source": [
        "Before getting started, a few general hints:\n",
        "\n",
        "1. `lambda` constructors: they are used in `python` to define anonymous functions ([see this link](https://realpython.com/python-lambda/))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCYW6lnX0rRV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "arr = np.array([1.5, 2.8, 3.1])\n",
        "scale = lambda x: x * 3\n",
        "scale(arr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlwVHKC4QCgC"
      },
      "source": [
        "2. Handle `pandas` dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPA9JSHJQUdn"
      },
      "outputs": [],
      "source": [
        "\n",
        "Image(url=\"https://www.tutorialspoint.com/python_pandas/images/structure_table.jpg\",width=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ta5_ta5iQefI"
      },
      "outputs": [],
      "source": [
        "# Empty dataframe\n",
        "df = pd.DataFrame()\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zseq-qWdQgEJ"
      },
      "outputs": [],
      "source": [
        "# Basic database\n",
        "data = [1,2,3,4,5]\n",
        "df = pd.DataFrame(data)\n",
        "print(df.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfzt1ZFAQkyd"
      },
      "outputs": [],
      "source": [
        "# Database with column labels\n",
        "data = [['Alex',10],['Bob',12],['Clarke',13]]\n",
        "df = pd.DataFrame(data,columns=['Name','Age'])\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnHJWEViQwCW"
      },
      "outputs": [],
      "source": [
        "# Repartitioning data into dataframe\n",
        "data = [['Alex',10],['Bob',12],['Clarke',13]]\n",
        "df = pd.DataFrame(data,columns=['Name','Age'],dtype=float)\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sI3X_pS6Qwjp"
      },
      "outputs": [],
      "source": [
        "# Deal with indices\n",
        "data = {'Name':['Tom', 'Jack', 'Steve', 'Ricky'],'Age':[28,34,29,42]}\n",
        "df = pd.DataFrame(data, index=['rank1','rank2','rank3','rank4'])\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXEd3Abh3dxD"
      },
      "source": [
        "3. plot data: ([see this link](https://matplotlib.org/tutorials/introductory/usage.html#sphx-glr-tutorials-introductory-usage-py))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjLz5ga_8crQ"
      },
      "outputs": [],
      "source": [
        "x = np.linspace(0, 2, 100)\n",
        "plt.plot(x, x, label='linear')\n",
        "plt.plot(x, x**2, label='quadratic')\n",
        "plt.plot(x, x**3, label='cubic')\n",
        "plt.xlabel('x label')\n",
        "plt.ylabel('y label')\n",
        "plt.title(\"Simple Plot\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqdFye6vWfYM"
      },
      "source": [
        "# **Exercise 1** Non-linear regressions of real functions with $\\mathcal{MLP}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHcUGwPMbAwT"
      },
      "source": [
        "The aim of this task is to get acquainted with non-linear polynomial regressions. In this task, the following *kindergarten* equation is considered:\n",
        "\n",
        "$$f(x) = \\sin(30 \\cdot (x-0.9)^4)\\cos(2 \\cdot (x-0.9))+\\frac{x-0.9}{2}$$\n",
        "\n",
        "Given the equation above, solve the following issues:\n",
        "\n",
        "## - Step 1 **[TODO]**: Plot $f(x)$ and evaluate it at $N$=100 random points $x_i\\sim\\mathcal{U}\\left[0,1\\right]$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsO6NkXVoIsV"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "# Grant reproductibility\n",
        "np.random.seed(0)\n",
        "N = 10\n",
        "x_plot = np.linspace(0, 1, N*100)\n",
        "rng = np.random.RandomState(0)\n",
        "x_train = np.sort(rng.choice(x_plot,\n",
        "                             size=N,\n",
        "                             replace=False))\n",
        "def f(x):\n",
        "    return # [TODO]\n",
        "y_train = f(x_train)\n",
        "\n",
        "# create 2D-array versions of these arrays to feed to transformers\n",
        "X_train = x_train[:, np.newaxis]\n",
        "X_plot = x_plot[:, np.newaxis]\n",
        "\n",
        "# plot function\n",
        "lw = 2\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_prop_cycle(\n",
        "    color=[\"black\", \"teal\", \"yellowgreen\", \"gold\", \"darkorange\", \"tomato\"]\n",
        ")\n",
        "ax.plot(x_plot, f(x_plot), linewidth=lw, label=\"ground truth\")\n",
        "\n",
        "# plot training points\n",
        "ax.scatter(x_train, y_train, label=\"training points\")\n",
        "ax.set_xlabel(\"x\")\n",
        "ax.set_ylabel(\"f(x)\")\n",
        "ax.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihkjOEf_cLvW"
      },
      "source": [
        "\n",
        "\n",
        "## - Step 2 **[TODO]**: Fit the selected points for different polynomial orders (hint: 3,4,5,... or piece-wise polynomials). Show the fitting improvements obtained when changing the polynomial order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lz6aBTurhLIX"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures, SplineTransformer\n",
        "\n",
        "lw = 2\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_prop_cycle(\n",
        "    color=[\"black\", \"teal\", \"yellowgreen\",\n",
        "           \"gold\", \"darkorange\", \"tomato\",\n",
        "           \"skyblue\"]\n",
        ")\n",
        "ax.plot(x_plot, f(x_plot), linewidth=lw, label=\"ground truth\")\n",
        "\n",
        "# plot training points\n",
        "ax.scatter(x_train, y_train, label=\"training points\")\n",
        "\n",
        "# polynomial features\n",
        "alpha = 1e-10# penalty coefficient\n",
        "for degree in [3, 4, 5, 9, N]:\n",
        "    model = make_pipeline(PolynomialFeatures(degree),\n",
        "                          Ridge(alpha=alpha))\n",
        "    # [TODO]\n",
        "    # Hint: use the sklearn function `fit` and `predict`\n",
        "    ax.plot(x_plot, y_plot, label=f\"degree {degree}\")\n",
        "\n",
        "\n",
        "ax.legend()\n",
        "ax.set_xlim(0, 1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73L6MJjZhKU9"
      },
      "source": [
        "\n",
        "## - Step 3 **[TODO]**: How the fit improves when considering 100, 1000 random points?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxKo-1bsoy_l"
      },
      "outputs": [],
      "source": [
        "N = 1000\n",
        "x_train = np.sort(rng.choice(x_plot,\n",
        "                             size=N,\n",
        "                             replace=False))\n",
        "y_train = f(x_train)\n",
        "X_train = x_train[:, np.newaxis]\n",
        "\n",
        "lw = 2\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_prop_cycle(\n",
        "    color=[\"black\", \"teal\", \"yellowgreen\",\n",
        "           \"gold\", \"darkorange\", \"tomato\",\n",
        "           \"skyblue\"]\n",
        ")\n",
        "ax.plot(x_plot, f(x_plot), linewidth=lw, label=\"ground truth\")\n",
        "\n",
        "# plot training points\n",
        "ax.scatter(x_train, y_train, label=\"training points\")\n",
        "# polynomial features\n",
        "alpha =0 # penalty coefficient\n",
        "for degree in [3, 4, 5, 9, N//2, N-1]:\n",
        "    model = make_pipeline(PolynomialFeatures(degree),\n",
        "                          Ridge(alpha=alpha))\n",
        "    # [TODO]\n",
        "    # Hint: use the sklearn function `fit` and `predict`\n",
        "    ax.plot(x_plot, y_plot, label=f\"degree {degree}\")\n",
        "\n",
        "\n",
        "ax.legend()\n",
        "ax.set_xlim(0, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bv66hKpozO-"
      },
      "source": [
        "## - Step 4 **[TODO]**:  Design a $\\mathcal{MLP}$ to fit the curve sampled with 10, 100, 1000 points respectively. Use the least number of layers and neurons possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWM9YcBFHTS5"
      },
      "source": [
        "## - Step 4.1 **[TODO]**:  create and split dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmvXrMPbHO8m"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAEE1-ops902"
      },
      "outputs": [],
      "source": [
        "'''[TODO] Create dataset'''\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "m = torch.distributions.Uniform(torch.zeros(1),\n",
        "                                torch.ones(1))\n",
        "\n",
        "N = 1000\n",
        "X = m.sample((N,)).to(torch.float32)\n",
        "# split into train and test sets\n",
        "\n",
        "# Hint: use the sklearn function `train_test_split`\n",
        "y = f(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.1,random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHll11ZbI4U7"
      },
      "source": [
        "## - Step 4.2 **[TODO]**:  design the $\\mathcal{MLP}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OvUF4A_8_ta"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "'''[TODO] Design MLP'''\n",
        "\n",
        "fan_in = 1\n",
        "h_theta = nn.Sequential(\n",
        "    nn.Linear(fan_in,4),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(4,1),\n",
        "    nn.Flatten()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyVUTJR8JBvN"
      },
      "source": [
        "## - Step 4.3 **[TODO]**:  choose the optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXtCCbeSwJCC"
      },
      "outputs": [],
      "source": [
        "'''[TODO] Optimization setup'''\n",
        "# Define the loss function\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# number of epochs\n",
        "n_e = 800\n",
        "\n",
        "# batch size and batch per epochs\n",
        "batch_size = 10\n",
        "batches_per_epoch = len(X_train) // batch_size\n",
        "\n",
        "# Optimizer\n",
        "learning_rate = 0.1\n",
        "beta1 = 0.9 # Adam coefficients\n",
        "beta2 = 0.999 # Adam coefficients\n",
        "epsilon = 1e-8 # tolerance\n",
        "optimizer = torcch.optim.Adam(h_theta.parameters(), lr= learning_rate, betas=(beta1, beta2), eps=epsilon)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2x-Z8NuJNDm"
      },
      "source": [
        "## - Step 4.4 **[TODO]**:  train the $\\mathcal{MLP}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFQeWdXIv9Hv"
      },
      "outputs": [],
      "source": [
        "'''[TODO] Train the hidden-layer MLP'''\n",
        "import tqdm\n",
        "train_loss_hist = []\n",
        "test_loss_hist = []\n",
        "for epoch in range(n_e):\n",
        "    epoch_loss = []\n",
        "    # set model in training mode\n",
        "    h_theta.train()\n",
        "\n",
        "    with tqdm.trange(batches_per_epoch, unit=\"batch\", mininterval=0) as bar:\n",
        "        bar.set_description(f\"Epoch {epoch}\")\n",
        "        for i in bar:\n",
        "            # take a batch\n",
        "            start = i * batch_size\n",
        "            X_batch = X_train[start:start+batch_size]\n",
        "            y_batch = y_train[start:start+batch_size]\n",
        "            # infer (forward)\n",
        "            y_pred = h_theta(X_batch).squeeze()\n",
        "            # compute the loss\n",
        "            loss = loss_fn(y_pred,y_batch.squeeze())\n",
        "            # reset previously saved gradients and empty the optimizer memory\n",
        "            optimizer.zero_grad()\n",
        "            # run backward propagation\n",
        "            # [TODO]\n",
        "            # update weights\n",
        "            # [TODO]\n",
        "            # compute and store metrics\n",
        "            epoch_loss.append(float(loss))\n",
        "\n",
        "    # set model in evaluation mode to infer the class in the test set\n",
        "    # without storing gradients for brackprop\n",
        "    h_theta.eval()\n",
        "    # infer the class over the test set\n",
        "    y_pred = h_theta(X_test).squeeze()\n",
        "    acc = float(loss_fn(y_pred, y_test))\n",
        "    train_loss_hist.append(np.mean(epoch_loss))\n",
        "    test_loss_hist.append(acc)\n",
        "    # print(f\"Epoch {epoch} validation: MSE={acc:.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WHopbt0JR7L"
      },
      "source": [
        "## - Step 4.5 **[TODO]**:  plot the learning curves and the comparison between test data and prediction from trained $\\mathcal{MLP}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaNtL74y14Oy"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(4,4))\n",
        "ax.semilogy(train_loss_hist,\n",
        "            color='b',\n",
        "            linewidth=1,\n",
        "            label=r\"train $h_{\\mathbf{\\theta}}(\\mathbf{x})$\")\n",
        "# [TODO] Plot test curve (use label=r\"test $h_{\\mathbf{\\theta}}(\\mathbf{x})$\"))\n",
        "ax.set_xlabel(\"epoch\")\n",
        "ax.set_ylabel(r\"$\\Vert f(x)-h_\\theta(x) \\Vert^2$\")\n",
        "ax.set_xlim(0,n_e)\n",
        "ax.set_ylim(1e-6,1e0)\n",
        "ax.legend(frameon=False, loc='upper center', bbox_to_anchor=(0.5, 1.0),ncol=2,)\n",
        "fig.savefig(\"mse_fx_compare.png\", dpi=300, bbox_inches=\"tight\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDEVttiz21hw"
      },
      "outputs": [],
      "source": [
        "h_theta.eval()\n",
        "\n",
        "lw = 2\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_prop_cycle(\n",
        "    color=[\"black\", \"teal\", \"yellowgreen\",\n",
        "           \"gold\", \"darkorange\", \"tomato\",\n",
        "           \"skyblue\"]\n",
        ")\n",
        "ax.plot(x_plot, f(x_plot), linewidth=lw, label=\"ground truth\")\n",
        "\n",
        "ax.scatter(X_test.cpu().numpy(),h_theta(X_test).detach().cpu().numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NR051wadwn7C"
      },
      "source": [
        "# **Exercise 2**: Approximate an approximately-radial function with $\\mathcal{MLP}$ (see Section 2.4, Chapter 7)\n",
        "\n",
        "## **Quick facts**\n",
        "\n",
        "\n",
        "- Despite the fact that Universal approximation theorem for a 1-hidden-layered perceptron (see Theorem 24, Chapter 8) proves the universal approximation capability of a 1-hidden-layer $\\mathcal{MLP}$, provided that enough neurons are considered, this result is quite hard to exploit in real applications, since the number of neurons can easily become too large to handle from a computational standpoint.\n",
        "\n",
        "- Eldan and Shamir [1] showed that it exists an approximately radial function $\\varphi(\\mathbf{x}):\\mathbb{R}^{d_X}\\to\\mathbb{R}$, $\\varphi: \\mathbf{x}\\mapsto\\varphi(\\Vert\\mathbf{x}\\Vert)$ that can be approximated by a ``small`` (bounded number of neurons) 2-hidden-layers $\\mathcal{MLP}$ with arbitrary accuracy, but that cannot be approximated by a 1-hidden-layer $\\mathcal{MLP}$ below a certain accuracy, unless the number of neurons $N_K$ grows exponentially with $d_X$.\n",
        "\n",
        "- In particular, this results is valid for any activation function $g$ and with no further constraint on the weights and biases adopted in the $\\mathcal{MLP}$ (on the contrary, the Universal Approximation Theorem requires that the high-frequency components $\\Vert \\mathbf{w}_n\\Vert$ are smaller than a constant).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### *Bibliography*:\n",
        "\n",
        "[1] Eldan, R.; Shamir, O. The Power of Depth for Feedforward Neural Networks. In Workshop and Conference Proceedings; PMLR, 2016; Vol. 49, pp 1--34. url: https://proceedings.mlr.press/v49/eldan16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7j1SBUCH-AZ7"
      },
      "source": [
        "## **Questions**\n",
        "\n",
        "- What kind of functions cannot be approximated with a *reasonable* accuracy by a $\\mathcal{MLP}$ of $N_\\ell$ layers?\n",
        "\n",
        "- How many neurons should be considered for each layer?\n",
        "\n",
        "- What is the effect of having a number of layers that is higher than the number of neurons per layer?\n",
        "\n",
        "## **Learning Outcomes**\n",
        "\n",
        "- This results proves that increasing the depth of the $\\mathcal{MLP}$ widens the approximation capability of the $\\mathcal{MLP}$ and that the depth of the $\\mathcal{MLP}$ should be privileged with the respect to its layer dimension (but being careful to avoid vanishing\n",
        "gradient problems)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ny6N7jz-HOs"
      },
      "source": [
        "## **Objective**\n",
        "\n",
        "In the following, you are asked to conceive a multi-layer feed-forward $\\mathcal{MLP}$ to approximate the following approximately-radial function and compare the accuracy obtained with 1-hidden layer and with 2-hidden layers. The approximately-radial function is defined as:\n",
        "\n",
        "$$\\varphi(\\mathbf{x}) = \\left(\\frac{R_{d_X}}{\\Vert\\mathbf{x}\\Vert}\\right)^{\\frac{d_X}{2}}J_{\\frac{d_X}{2}}(2\\pi R_{d_X}\\Vert\\mathbf{x}\\Vert)  =\\int_{\\mathbf{w}:\\Vert\\mathbf{w}\\Vert\\le R_d} e^{-2\\pi i \\langle \\mathbf{x}, \\mathbf{w}\\rangle} d\\mathbf{w}\\phantom{}^{(1)}\n",
        "$$\n",
        "\n",
        "with $J_{\\frac{d_X}{2}}(2\\pi R_{d_X}\\Vert\\mathbf{x}\\Vert)$ the Bessel function of first kind of order $\\frac{d_X}{2}$. In the following, consider $d_X=2$.\n",
        "\n",
        "**$\\phantom{}^{(1)}$Note**: The approximately radial function is the inverse Fourier transform of the indicator function on a unit volume euclidean ball $B_{R_{d_X}}(0)$, of radius $R_{d_X}$ such that its volume $V_{d_X}(R_{d_X}) = 1$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuepuQKsCv8L"
      },
      "source": [
        "## - Step 1: compute and plot the approximately-radial function $\\varphi$ on, restricted to a compact set $\\mathcal{X}_{\\square}=\\left[-l_x,l_x\\right]\\times\\left[-l_y,l_y\\right]\\subset\\mathbb{R}^2$ over a discrete regular grid of $n_x\\times n_y$ points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LYWqFGgksDx"
      },
      "outputs": [],
      "source": [
        "'''Define grid in R2'''\n",
        "lx = 10.0\n",
        "ly = 10.0\n",
        "x_lim = np.array([-lx,lx], dtype=np.float64) # x-bounds\n",
        "y_lim = np.array([-ly,ly], dtype=np.float64) # y-bounds\n",
        "\n",
        "nx = 1001 # number of grid points along x\n",
        "ny = 1001 # number of grid points along y\n",
        "# vector of coordinates along x\n",
        "xv = np.linspace(x_lim[0], x_lim[1], nx, endpoint=True)\n",
        "# vector of coordinates along y\n",
        "yv = np.linspace(y_lim[0], y_lim[1], ny, endpoint=True)\n",
        "# define mesh grid\n",
        "yg, xg = np.meshgrid(yv,xv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yjldGWsEwvP"
      },
      "source": [
        "## - Step 2: evaluate $\\varphi$ over the grid and plot it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjNevvVGks5P"
      },
      "outputs": [],
      "source": [
        "'''Define varphi(x,y) over the grid'''\n",
        "from scipy.special import jv # Bessel function\n",
        "d_X = 2 # dimension of the problem\n",
        "R_d = 0.2 # radius\n",
        "phi = (R_d/(np.sqrt(xg**2+yg**2)))**(int(d_X//2))*jv(int(d_X//2),\n",
        "                                                     2.0*np.pi*R_d*np.sqrt(xg**2+\n",
        "                                                                           yg**2))\n",
        "fig = plt.figure(figsize=(4,4),)\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(xg, yg, phi, cmap=cm.jet)\n",
        "ax.set_xlabel(r\"$x$\")\n",
        "ax.set_ylabel(r\"$y$\")\n",
        "ax.set_zlabel(r\"$\\varphi(x,y)$\")\n",
        "fig.savefig(\"radial_function.png\", dpi=300, bbox_inches=\"tight\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKUzqPK9Gsxz"
      },
      "source": [
        "## - Step 3 **[TODO]**: create the training and test datasets by uniformly sampling the function $\\varphi$ over $\\mathcal{X}_\\square$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NefRB4EZk0s_"
      },
      "outputs": [],
      "source": [
        "''' [TODO] Create uniformly random grid of points for training'''\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Grant reproductibility\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Hint: use the package torch.distributions\n",
        "# https://pytorch.org/docs/stable/distributions.html\n",
        "# for uniform distribution\n",
        "m = torch.distributions # [TODO]\n",
        "X = m.sample((4000,)).to(torch.float32)\n",
        "y = (R_d/(np.sqrt(X[:,0]**2+X[:,1]**2)))**(int(d_X//2))*jv(int(d_X//2),\n",
        "                                                           2.0*np.pi*R_d*np.sqrt(\n",
        "                                                              X[:,0]**2+\n",
        "                                                              X[:,1]**2))\n",
        "\n",
        "# split into train and test sets\n",
        "# Hint: use the sklearn function `train_test_split`\n",
        " # [TODO]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAw0obKNKwpZ"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(9,4),)\n",
        "ax = fig.add_subplot(121, projection='3d')\n",
        "ax.plot_surface(xg, yg, phi, cmap=cm.jet, alpha=0.3)\n",
        "ax.scatter(X_train[:,0], X_train[:,1], y_train, color='k', s=5)\n",
        "ax.set_xlabel(r\"$x$\")\n",
        "ax.set_ylabel(r\"$y$\")\n",
        "ax.set_zlabel(r\"$\\varphi(x,y)$\")\n",
        "ax.set_title(\"Train dataset\")\n",
        "\n",
        "ax = fig.add_subplot(122, projection='3d')\n",
        "ax.plot_surface(xg, yg, phi, cmap=cm.jet, alpha=0.3)\n",
        "ax.scatter(X_test[:,0], X_test[:,1], y_test, color='k', s=5)\n",
        "\n",
        "ax.set_xlabel(r\"$x$\")\n",
        "ax.set_ylabel(r\"$y$\")\n",
        "ax.set_zlabel(r\"$\\varphi(x,y)$\")\n",
        "ax.set_title(\"Test dataset\")\n",
        "fig.savefig(\"train_test_radial_function.png\", dpi= 300, bbox_inches=\"tight\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GA3GAzrkH6-k"
      },
      "source": [
        "## - Step 4 **[TODO]**: Define a 1-hidden layer $\\mathcal{MLP}$ $h_\\theta^{(1)}$ and a 2-hidden layers $\\mathcal{MLP}$ $h_\\theta^{(2)}$ for further comparisons, using the `torch.nn` package. Adopt an adequate number of of hidden neurons for both cases.\n",
        "\n",
        "**Note**: use $ReLU$ activation functions and linear output layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8Y-hKUgDKlE"
      },
      "outputs": [],
      "source": [
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eqk2UGjkvev"
      },
      "outputs": [],
      "source": [
        "'''[TODO] Define 1-hidden-layer MLP selecting an adequate fan_in'''\n",
        "fan_in = 10000\n",
        "h_theta1 = nn.Sequential(\n",
        " # [TODO]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUaK3Bq-kyvn"
      },
      "outputs": [],
      "source": [
        "'''[TODO] Define 2-hidden-layer MLP'''\n",
        "fan_in = 100\n",
        "h_theta2 = nn.Sequential(\n",
        " # [TODO]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93WvyG4xI9T4"
      },
      "source": [
        "## - Step 5 **[TODO]**: Define the best optimizer for both $h_\\theta^{(1)}$ and $h_\\theta^{(2)}$. Choose the learning rate accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cobc_jmUIoHw"
      },
      "outputs": [],
      "source": [
        "''' [TODO] Define the optimizer for 1-hidden-layer MLP'''\n",
        "learning_rate1 = 0.0001\n",
        "optimizer1 =  # [TODO]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTzr2nHhHuf4"
      },
      "outputs": [],
      "source": [
        "''' [TODO] Define the optimizer for 2-hidden-layers MLP'''\n",
        "learning_rate2 = 0.0001\n",
        "optimizer2 =  # [TODO]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bg96YTMlK7rf"
      },
      "source": [
        "## - Step 6 **[TODO]**: setup the gradient descent strategy and the loss function. Optional: initialize the weights according to the best strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDzjZxEMk5C3"
      },
      "outputs": [],
      "source": [
        "''' [TODO] Gradient descent set up and loss function'''\n",
        "# Define the loss function\n",
        "loss_fn =  # [TODO]\n",
        "\n",
        "# number of epochs\n",
        "n_e = 1000\n",
        "\n",
        "# batch size and batch per epochs\n",
        "batch_size = 200\n",
        "batches_per_epoch = len(X_train) // batch_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HFP4U7cLr__"
      },
      "source": [
        "## - Step 7 **[TODO]**: Train $h_\\theta^{(1)}$ and $h_\\theta^{(2)}$ and track the approximation error for training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_q9Potwk7LP"
      },
      "outputs": [],
      "source": [
        "''' [TODO] Train 1-hidden-layer MLP'''\n",
        "import tqdm\n",
        "train_loss_hist1 = []\n",
        "test_loss_hist1 = []\n",
        "for epoch in range(n_e):\n",
        "    epoch_loss = []\n",
        "    # set model in training mode\n",
        "    h_theta1.train()\n",
        "\n",
        "    with tqdm.trange(batches_per_epoch, unit=\"batch\", mininterval=0) as bar:\n",
        "        bar.set_description(f\"Epoch {epoch}\")\n",
        "        for i in bar:\n",
        "            # take a batch\n",
        "            start = i * batch_size\n",
        "            X_batch = X_train[start:start+batch_size]\n",
        "            y_batch = y_train[start:start+batch_size]\n",
        "            # infer (forward)\n",
        "            y_pred =  # [TODO]\n",
        "            # compute the loss\n",
        "            loss =  # [TODO]\n",
        "            # reset previously saved gradients and empty the optimizer memory\n",
        "            optimizer1.zero_grad()\n",
        "            # run backward propagation\n",
        "            loss.backward()\n",
        "            # update weights\n",
        "            optimizer1.step()\n",
        "            # compute and store metrics\n",
        "            epoch_loss.append(float(loss))\n",
        "\n",
        "    # set model in evaluation mode to infer the class in the test set\n",
        "    # without storing gradients for brackprop\n",
        "    h_theta1.eval()\n",
        "    # infer the class over the test set\n",
        "    y_pred = h_theta1(X_test).squeeze()\n",
        "    acc = float(loss_fn(y_pred, y_test))\n",
        "    train_loss_hist1.append(np.mean(epoch_loss))\n",
        "    test_loss_hist1.append(acc)\n",
        "    print(f\"Epoch {epoch} validation: MSE={acc:.1f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aESnHNzk9kn"
      },
      "outputs": [],
      "source": [
        "'''[TODO] Train 2-hidden-layer MLP'''\n",
        "import tqdm\n",
        "train_loss_hist2 = []\n",
        "test_loss_hist2 = []\n",
        "for epoch in range(n_e):\n",
        "    epoch_loss = []\n",
        "    # set model in training mode\n",
        "    h_theta2.train()\n",
        "\n",
        "    with tqdm.trange(batches_per_epoch, unit=\"batch\", mininterval=0) as bar:\n",
        "        bar.set_description(f\"Epoch {epoch}\")\n",
        "        for i in bar:\n",
        "            # take a batch\n",
        "            start = i * batch_size\n",
        "            X_batch = X_train[start:start+batch_size]\n",
        "            y_batch = y_train[start:start+batch_size]\n",
        "            # infer (forward)\n",
        "            y_pred = h_theta2(X_batch).squeeze()\n",
        "            # compute the loss\n",
        "            loss = loss_fn(y_pred, y_batch)\n",
        "            # reset previously saved gradients and empty the optimizer memory\n",
        "            # [TODO]\n",
        "            # run backward propagation\n",
        "            # [TODO]\n",
        "            # update weights\n",
        "            optimizer2.step()\n",
        "            # compute and store metrics\n",
        "            epoch_loss.append(float(loss))\n",
        "\n",
        "    # set model in evaluation mode to infer the class in the test set\n",
        "    # without storing gradients for brackprop\n",
        "    h_theta2.eval()\n",
        "    # infer the class over the test set\n",
        "    y_pred = h_theta2(X_test).squeeze()\n",
        "    acc = float(loss_fn(y_pred, y_test))\n",
        "    train_loss_hist2.append(np.mean(epoch_loss))\n",
        "    test_loss_hist2.append(acc)\n",
        "    # print(f\"Epoch {epoch} validation: Accuracy={acc:.1f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_W1zPfRnk_tb"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(4,4))\n",
        "ax.semilogy(train_loss_hist1,\n",
        "            color='b',\n",
        "            linewidth=1,\n",
        "            label=r\"train $h^{1}_{\\mathbf{\\theta}}(\\mathbf{x})$\")\n",
        "ax.semilogy(test_loss_hist1,\n",
        "            color='r',\n",
        "            linewidth=1,\n",
        "            label=r\"test $h^{1}_{\\mathbf{\\theta}}(\\mathbf{x})$\")\n",
        "ax.semilogy(train_loss_hist2,\n",
        "            color='steelblue',\n",
        "            linewidth=3,\n",
        "            label=r\"train $h^{2}_{\\mathbf{\\theta}}(\\mathbf{x})$\")\n",
        "ax.semilogy(test_loss_hist2,\n",
        "            color='orange',\n",
        "            linewidth=3,\n",
        "            label=r\"test $h^{2}_{\\mathbf{\\theta}}(\\mathbf{x})$\")\n",
        "ax.set_xlabel(\"epoch\")\n",
        "ax.set_ylabel(r\"$\\Vert \\varphi(x,y)-h_{\\mathbf{\\theta}}(\\mathbf{x}) \\Vert^2$\")\n",
        "ax.set_xlim(0,n_e)\n",
        "ax.set_ylim(1e-6,1e0)\n",
        "ax.legend(frameon=False, loc='upper center', bbox_to_anchor=(0.5, 1.0),ncol=2,)\n",
        "fig.savefig(\"mse_radial_compare.png\", dpi=300, bbox_inches=\"tight\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zWTAbXvlB3f"
      },
      "outputs": [],
      "source": [
        "h_theta2.eval()\n",
        "fig = plt.figure(figsize=(4,4),)\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "# ax.plot_surface(xg, yg, phi, cmap=cm.jet, alpha=0.3)\n",
        "ax.scatter(X_train[:,0], X_train[:,1], y_train, color='k', s=30)\n",
        "ax.scatter(X_test[:,0], X_test[:,1], y_test, color='orange', s=30)\n",
        "ax.scatter(X_train[:,0], X_train[:,1],\n",
        "           h_theta2(X_train).detach().cpu().numpy(), color='k', s=30, marker=\"x\")\n",
        "ax.scatter(X_test[:,0], X_test[:,1],\n",
        "           h_theta2(X_test).detach().cpu().numpy(), color='orange', s=30, marker=\"x\")\n",
        "\n",
        "ax.set_xlabel(r\"$x$\")\n",
        "ax.set_ylabel(r\"$y$\")\n",
        "ax.set_zlabel(r\"$\\varphi(x,y)$\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSHEVb3oMN47"
      },
      "source": [
        "## - Step 8 **[TODO]**: what is the best traded-off between total number of weights, training time, approximation error?\n",
        "\n",
        "Try different solutions !"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}